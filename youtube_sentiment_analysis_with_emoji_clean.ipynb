{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzp_dPURGv04"
      },
      "source": [
        "# import\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "id": "17UAyut7Glkk",
        "outputId": "60f39a38-f8c2-4e54-a61e-d9471840fe45"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "id": "vJ-AeYbZK49J",
        "outputId": "6ea23db3-a5e1-4427-99e7-c41f1a2f096b"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/uclnlp/emoji2vec.git\n",
        "!cd emoji2vec\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!pip install gensim\n",
        "# Step 1. Clone repo 並安裝相依套件\n",
        "!git clone https://github.com/WING-NUS/ELCo.git\n",
        "%cd ELCo\n",
        "!pip install -r /content/ELCo/scripts/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w14gQ1hjIVsY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/final_balanced_training_set(1).csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/final_test_set_processed(1).csv')\n",
        "label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "train_df[\"Sentiment\"] = train_df[\"Sentiment\"].map(label_map)\n",
        "test_df[\"Sentiment\"] = test_df[\"Sentiment\"].map(label_map)\n",
        "train_df[\"Sentiment\"] = train_df[\"Sentiment\"].astype(int)\n",
        "test_df[\"Sentiment\"] = test_df[\"Sentiment\"].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO_tyyvGHT1t"
      },
      "source": [
        "# only text\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 569
        },
        "id": "ID9BhF05HdAi",
        "outputId": "8c1992bd-97a5-4c47-93e6-607035951cab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === 資料集定義 ===\n",
        "class TextOnlyDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        self.texts = dataframe[\"CommentText\"].tolist()\n",
        "        self.labels = dataframe[\"Sentiment\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "# === 評估指標 ===\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    macro_f1 = f1_score(labels, preds, average='macro')\n",
        "    precision = precision_score(labels, preds, average='macro')\n",
        "    recall = recall_score(labels, preds, average='macro')\n",
        "\n",
        "    try:\n",
        "        y_true_bin = label_binarize(labels, classes=list(range(logits.shape[1])))\n",
        "        auc = roc_auc_score(y_true=y_true_bin, y_score=logits, average='macro', multi_class='ovr')\n",
        "    except ValueError:\n",
        "        auc = 0.0\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"auc\": auc\n",
        "    }\n",
        "\n",
        "# === 訓練參數與模型設定 ===\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
        "\n",
        "# 載入資料（你要自行準備 train_df, test_df）\n",
        "# train_df = pd.read_csv(\"train.csv\")\n",
        "# test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "train_dataset = TextOnlyDataset(train_df, tokenizer)\n",
        "test_dataset = TextOnlyDataset(test_df, tokenizer)\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"  # 防止自動連線到 wandb\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_text_only\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir=\"./logs_text_only\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# === 開始訓練與評估 ===\n",
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(results)\n",
        "\n",
        "# === 推論 ===\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "texts = test_df[\"CommentText\"].tolist()\n",
        "\n",
        "# Emoji 抽取\n",
        "def extract_emojis(text):\n",
        "    return ' '.join(re.findall(r\"[^\\w\\s,.!?\\'\\\"@#$%^&*()<>+=:;~`]+\", text))\n",
        "\n",
        "emoji_lists = [extract_emojis(t) for t in texts]\n",
        "\n",
        "# 推論\n",
        "for batch in tqdm(test_loader, desc=\"🔍 Predicting\"):\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"labels\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "    true_labels.extend(labels.cpu().tolist())\n",
        "    pred_labels.extend(preds.cpu().tolist())\n",
        "\n",
        "# 輸出結果\n",
        "df_result = pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"pred_label\": pred_labels,\n",
        "    \"emoji_list\": emoji_lists\n",
        "})\n",
        "\n",
        "df_result.to_csv(\"youtube_emoji_sentiment_onlytext.csv\", index=False)\n",
        "print(\"✅ 匯出完成：youtube_emoji_sentiment_onlytext.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T9vGlJoHWhX"
      },
      "source": [
        "# emoji2vec"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 1000
        },
        "id": "nohz3zeyHHG4",
        "outputId": "8a289279-f193-412b-e724-22000a08d4d4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Now try importing gensim and KeyedVectors again\n",
        "from gensim.models import KeyedVectors\n",
        "import re\n",
        "emoji_model = KeyedVectors.load_word2vec_format(\"/content/emoji2vec/pre-trained/emoji2vec.bin\", binary=True)\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/final_balanced_training_set(1).csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/final_test_set_processed(1).csv')\n",
        "# 範例：取得 😂 的向量\n",
        "vec = emoji_model['😂']\n",
        "print(vec.shape)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def get_emoji_vector(emojis):\n",
        "    vectors = [emoji_model[emoji] for emoji in emojis if emoji in emoji_model]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(emoji_model.vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "emoji_pattern = re.compile(\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
        "\n",
        "\n",
        "train_df[\"emojis\"] = train_df[\"CommentText\"].apply(lambda text: emoji_pattern.findall(str(text)))\n",
        "test_df[\"emojis\"] = test_df[\"CommentText\"].apply(lambda text: emoji_pattern.findall(str(text)))\n",
        "train_df[\"emoji_vec\"] = train_df[\"emojis\"].apply(get_emoji_vector)\n",
        "test_df[\"emoji_vec\"] = test_df[\"emojis\"].apply(get_emoji_vector)\n",
        "label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "\n",
        "train_df[\"Sentiment\"] = train_df[\"Sentiment\"].map(label_map)\n",
        "test_df[\"Sentiment\"] = test_df[\"Sentiment\"].map(label_map)\n",
        "\n",
        "print(train_df.head())\n",
        "## roberta + emoji2vec\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel\n",
        "from sklearn.metrics import accuracy_score, f1_score , precision_score, recall_score, roc_auc_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Define the dataset class as before\n",
        "class EmojiSentimentDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, task=\"Sentiment\", max_length=128):\n",
        "        self.texts = dataframe[\"CommentText\"].tolist()\n",
        "        self.emoji_vecs = dataframe[\"emoji_vec\"].tolist()\n",
        "        self.labels = dataframe[task].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        emoji_vector_data = self.emoji_vecs[idx]\n",
        "        if not isinstance(emoji_vector_data, (list, np.ndarray)):\n",
        "\n",
        "             print(f\"Warning: Invalid emoji_vec at index {idx}. Using zero vector.\")\n",
        "             emoji_vector = torch.zeros(300, dtype=torch.float32)\n",
        "        else:\n",
        "            emoji_vector = torch.tensor(emoji_vector_data, dtype=torch.float32)\n",
        "\n",
        "        label_value = self.labels[idx]\n",
        "\n",
        "        if not isinstance(label_value, int):\n",
        "             print(f\"Warning: Invalid label at index {idx}. Skipping this item or using a default label if appropriate.\")\n",
        "\n",
        "             raise ValueError(f\"Invalid label type at index {idx}: {type(label_value)}\")\n",
        "\n",
        "\n",
        "        label = torch.tensor(label_value, dtype=torch.long)\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"emoji_vec\": emoji_vector,\n",
        "            \"labels\": label\n",
        "        }\n",
        "\n",
        "# Define the model, metrics, and collate_fn as before\n",
        "class RobertaWithEmoji(nn.Module):\n",
        "    def __init__(self, num_labels=3, emoji_dim=300):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.roberta.config.hidden_size + emoji_dim, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emoji_vec, labels=None): # Add labels to forward for Trainer to pass it\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]  # 取 [CLS] 向量\n",
        "        combined = torch.cat((cls_output, emoji_vec), dim=1)\n",
        "        logits = self.classifier(self.dropout(combined))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
        "\n",
        "        return (loss, logits) if loss is not None else logits\n",
        "\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    macro_f1 = f1_score(labels, preds, average='macro')\n",
        "    precision = precision_score(labels, preds, average='macro')\n",
        "    recall = recall_score(labels, preds, average='macro')\n",
        "\n",
        "    try:\n",
        "        labels = np.array(labels)\n",
        "        n_classes = logits.shape[1]\n",
        "        y_true_bin = label_binarize(labels, classes=list(range(n_classes)))\n",
        "\n",
        "        auc = roc_auc_score(\n",
        "            y_true=y_true_bin,\n",
        "            y_score=logits,\n",
        "            multi_class='ovr',\n",
        "            average='macro'\n",
        "        )\n",
        "    except ValueError as e:\n",
        "        print(\"⚠️ ROC AUC 計算錯誤:\", e)\n",
        "        auc = 0.0\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"auc\": auc\n",
        "    }\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "\n",
        "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
        "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
        "    emoji_vec = torch.stack([item[\"emoji_vec\"] for item in batch])\n",
        "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"emoji_vec\": emoji_vec,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "\n",
        "initial_test_rows = len(test_df)\n",
        "test_df = test_df[test_df[\"emoji_vec\"].notnull()]\n",
        "test_df = test_df[test_df[\"emoji_vec\"].apply(lambda x: isinstance(x, (list, np.ndarray)) and len(x) == 300)]\n",
        "cleaned_test_rows = len(test_df)\n",
        "print(f\"Cleaned test_df: Removed {initial_test_rows - cleaned_test_rows} rows with invalid emoji_vec. Remaining rows: {cleaned_test_rows}\")\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "test_dataset = EmojiSentimentDataset(test_df, tokenizer, task=\"Sentiment\")\n",
        "train_dataset = EmojiSentimentDataset(train_df, tokenizer, task=\"Sentiment\")\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = RobertaWithEmoji(num_labels=3).to(device)\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    do_eval=True,\n",
        "    do_train=True,\n",
        "    learning_rate=2e-5,\n",
        "    eval_strategy=\"epoch\",  # ✅ 每個 epoch 都 evaluate\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(\"📊 評估結果：\", results)\n",
        "# 切換到評估模式\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 準備 Dataloader\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "# 儲存欄位\n",
        "texts = test_df['CommentText'].tolist()\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "emoji_lists = []\n",
        "\n",
        "# 預測 loop\n",
        "for i, batch in enumerate(tqdm(test_loader, desc=\"🔍 Predicting\")):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    emoji_vec = batch['emoji_vec'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, emoji_vec=emoji_vec)\n",
        "        logits = outputs[\"logits\"]\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "    true_labels.extend(labels.cpu().tolist())\n",
        "    pred_labels.extend(preds.cpu().tolist())\n",
        "\n",
        "# 抽取 emoji 函數\n",
        "def extract_emojis(text):\n",
        "    return ' '.join(re.findall(r\"[^\\w\\s,.!?\\'\\\"@#$%^&*()<>+=:;~`]+\", text))\n",
        "\n",
        "emoji_lists = [extract_emojis(t) for t in texts]\n",
        "\n",
        "# 建立 DataFrame 並匯出\n",
        "df_result = pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"pred_label\": pred_labels,\n",
        "    \"emoji_list\": emoji_lists\n",
        "})\n",
        "\n",
        "df_result.to_csv(\"youtube_emoji_sentiment_emoji2vec.csv\", index=False)\n",
        "print(\"✅ 匯出完成：youtube_emoji_sentiment_emoji2vec.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtl1JpEPItUZ"
      },
      "source": [
        "# RoBERTa + emoji2vec(position)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 1000
        },
        "id": "WRoMVAEwJyeQ",
        "outputId": "145890e7-def8-4927-b502-c8ac02643b5a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from gensim.models import KeyedVectors\n",
        "from transformers import BertTokenizer, BertModel, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/final_balanced_training_set(1).csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/final_test_set_processed(1).csv')\n",
        "\n",
        "emoji_model = KeyedVectors.load_word2vec_format(\"/content/emoji2vec/pre-trained/emoji2vec.bin\", binary=True)\n",
        "\n",
        "def get_emoji_vector(emojis):\n",
        "    vectors = [emoji_model[emoji] for emoji in emojis if emoji in emoji_model]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(emoji_model.vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "emoji_pattern = re.compile(\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
        "\n",
        "\n",
        "sentiment_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "train_df[\"Sentiment\"] = train_df[\"Sentiment\"].map(sentiment_map)\n",
        "test_df[\"Sentiment\"] = test_df[\"Sentiment\"].map(sentiment_map)\n",
        "\n",
        "def preprocess_df(df):\n",
        "    emoji_vecs = []\n",
        "    emoji_positions = []\n",
        "    for _, row in df.iterrows():\n",
        "        text = str(row[\"CommentText\"])\n",
        "        emojis = emoji_pattern.findall(text)\n",
        "        emoji_combo = ''.join(emojis)\n",
        "        vec = get_emoji_vector(emojis)\n",
        "        emoji_vecs.append(vec)\n",
        "        idx = text.find(emoji_combo)\n",
        "        ratio = idx / max(1, len(text))\n",
        "        emoji_positions.append(0 if ratio < 0.5 else 1)\n",
        "    df[\"emoji2vec_vec\"] = emoji_vecs\n",
        "    df[\"emoji_pos\"] = emoji_positions\n",
        "    return df.dropna(subset=[\"Sentiment\"])\n",
        "\n",
        "train_df = preprocess_df(train_df)\n",
        "test_df = preprocess_df(test_df)\n",
        "\n",
        "class EmojiSentimentDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.texts = df[\"CommentText\"].tolist()\n",
        "        self.emoji2vec_vecs = df[\"emoji2vec_vec\"].tolist()\n",
        "        self.emoji_pos = df[\"emoji_pos\"].tolist()\n",
        "        self.labels = df[\"Sentiment\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        emoji2vec_vec = torch.tensor(self.emoji2vec_vecs[idx], dtype=torch.float32)\n",
        "        emoji_pos = torch.tensor(self.emoji_pos[idx], dtype=torch.long)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"emoji2vec_vec\": emoji2vec_vec,\n",
        "            \"emoji_pos\": emoji_pos,\n",
        "            \"labels\": label\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n",
        "    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch])\n",
        "    emoji2vec_vec = torch.stack([b[\"emoji2vec_vec\"] for b in batch])\n",
        "    emoji_pos = torch.stack([b[\"emoji_pos\"] for b in batch])\n",
        "    labels = torch.stack([b[\"labels\"] for b in batch])\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"emoji2vec_vec\": emoji2vec_vec,\n",
        "        \"emoji_pos\": emoji_pos,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "class BertWithEmojiOnly(nn.Module):\n",
        "    def __init__(self, num_labels=3, emoji2vec_dim=300, pos_embed_dim=16):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"roberta-base\")\n",
        "        self.pos_embedding = nn.Embedding(2, pos_embed_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        combined_dim = self.bert.config.hidden_size + emoji2vec_dim + pos_embed_dim\n",
        "        self.classifier = nn.Linear(combined_dim, num_labels)\n",
        "    def forward(self, input_ids, attention_mask, emoji2vec_vec, emoji_pos, labels=None):\n",
        "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_vec = bert_out.last_hidden_state[:, 0, :]\n",
        "        pos_vec = self.pos_embedding(emoji_pos)\n",
        "        concat_vec = torch.cat([cls_vec, emoji2vec_vec, pos_vec], dim=1)\n",
        "        output = self.dropout(concat_vec)\n",
        "        logits = self.classifier(output)\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return loss, logits\n",
        "        return logits\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    precision = precision_score(labels, preds, average=\"macro\")\n",
        "    recall = recall_score(labels, preds, average=\"macro\")\n",
        "    try:\n",
        "        from sklearn.preprocessing import label_binarize\n",
        "        y_true_bin = label_binarize(labels, classes=[0, 1, 2])\n",
        "        auc = roc_auc_score(y_true_bin, logits, multi_class=\"ovr\", average=\"macro\")\n",
        "    except:\n",
        "        auc = 0.0\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall, \"auc\": auc}\n",
        "\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "train_dataset = EmojiSentimentDataset(train_df, tokenizer)\n",
        "test_dataset = EmojiSentimentDataset(test_df, tokenizer)\n",
        "model = BertWithEmojiOnly().to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuned-emote-roberta\",\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    overwrite_output_dir=True,\n",
        "    learning_rate=2e-5\n",
        "    eval_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation results:\", eval_results)\n",
        "\n",
        "# 切換到評估模式\n",
        "model.eval()\n",
        "\n",
        "# 準備 Dataloader\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "# 儲存欄位\n",
        "texts = test_df['CommentText'].tolist()\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "emoji_lists = []\n",
        "\n",
        "# 預測 loop\n",
        "for i, batch in enumerate(test_loader):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    # Change 'emoji_vec' to 'emoji2vec_vec' to match the model's forward method\n",
        "    emoji2vec_vec = batch['emoji2vec_vec'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    with torch.no_grad():\n",
        "        # Pass emoji2vec_vec using the correct keyword argument name\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, emoji2vec_vec=emoji2vec_vec, emoji_pos=batch['emoji_pos'].to(device))\n",
        "        logits = outputs if isinstance(outputs, torch.Tensor) else outputs[1]\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "    true_labels.extend(labels.cpu().tolist())\n",
        "    pred_labels.extend(preds.cpu().tolist())\n",
        "\n",
        "# 抽取 emoji 函數\n",
        "def extract_emojis(text):\n",
        "    return ' '.join(re.findall(r\"[^\\w\\s,.!?\\'\\\"@#$%^&*()<>+=:;~`]+\", text))\n",
        "\n",
        "emoji_lists = [extract_emojis(t) for t in texts]\n",
        "\n",
        "# 建立 DataFrame 並匯出\n",
        "df_result = pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"pred_label\": pred_labels,\n",
        "    \"emoji_list\": emoji_lists\n",
        "})\n",
        "\n",
        "df_result.to_csv(\"youtube_emoji_sentiment_position.csv\", index=False)\n",
        "print(\"✅ 匯出完成：youtube_emoji_sentiment_position.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjcZYMaTJ2s5"
      },
      "source": [
        "# pretrain"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 706,
          "referenced_widgets": [
            "54dce249f5394c3ba9057783bd321d91",
            "ca0d08083ac24710a809c432a9eb4b7d",
            "dc8c011f36b8438d9b2b09414ab17299",
            "d6c4b6a0414842249df3ab479943af08",
            "f7bfa4092d504cdaa78752a52d1cd442",
            "73214fc27c4143bfbcab065acca71b56",
            "d1eb4558cb91454baeed6d6345f52405",
            "fe869bb635f14b16b01f78856836d588",
            "eb1223689ae144858f41dd83cea0dd05",
            "c2003a7096d546c7801344e9fca4b511",
            "1e222a7cb52a435791af334cef1505ce"
          ]
        },
        "id": "U2VVnT-N0XtP",
        "outputId": "4e06e542-8fc5-4117-bd49-9bcd28bb6c01"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset\n",
        "from gensim.models import KeyedVectors\n",
        "from transformers import (\n",
        "    RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling,\n",
        "    TrainingArguments, Trainer, RobertaModel\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "\n",
        "# ========== 1. Load emoji2vec ==========\n",
        "emoji_model = KeyedVectors.load_word2vec_format(\"/content/emoji2vec/pre-trained/emoji2vec.bin\", binary=True)\n",
        "\n",
        "def get_emoji_vector(text):\n",
        "    emojis = [ch for ch in text if ch in emoji_model]\n",
        "    if not emojis:\n",
        "        return np.zeros(emoji_model.vector_size)\n",
        "    vectors = [emoji_model[em] for em in emojis]\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# ========== 2. ELCo MLM 預訓練 ==========\n",
        "elco_df = pd.read_csv(\"/content/ELCo/ELCo.csv\")\n",
        "import random\n",
        "\n",
        "templates = [\n",
        "    lambda em, en: f\"{em} usually means {en}.\",\n",
        "    lambda em, en: f\"When people use {em}, they are likely feeling {en}.\",\n",
        "    lambda em, en: f\"{em} is often used to show {en}.\",\n",
        "    lambda em, en: f\"Using {em} might suggest that someone is experiencing {en}.\",\n",
        "    lambda em, en: f\"The emoji {em} stands for {en} in many cases.\",\n",
        "]\n",
        "\n",
        "elco_df['text'] = [random.choice(templates)(em, en) for em, en in zip(elco_df[\"EM\"], elco_df[\"EN\"])]\n",
        "elco_dataset = Dataset.from_pandas(elco_df[['text']])\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "def tokenize_mlm(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_elco = elco_dataset.map(tokenize_mlm, batched=True)\n",
        "tokenized_elco.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "mlm_model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
        "\n",
        "mlm_args = TrainingArguments(\n",
        "    output_dir=\"./roberta_elco_mlm\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    prediction_loss_only=True,\n",
        "    overwrite_output_dir=True,\n",
        "    learning_rate=1e-4\n",
        ")\n",
        "\n",
        "mlm_trainer = Trainer(model=mlm_model, args=mlm_args, train_dataset=tokenized_elco, data_collator=data_collator)\n",
        "mlm_trainer.train()\n",
        "mlm_model.save_pretrained(\"./roberta_elco_mlm\")\n",
        "tokenizer.save_pretrained(\"./roberta_elco_mlm\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN5DTjyu7MT-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== 3. EmoTE fine-tune with YouTube 自製資料集 ==========\n",
        "from datasets import Dataset\n",
        "\n",
        "yt_emote_df = pd.read_csv(\"/content/drive/MyDrive/YouTube_Emote_Combined.csv\")\n",
        "yt_emote_dataset = Dataset.from_pandas(yt_emote_df)\n",
        "\n",
        "def tokenize_emote(example):\n",
        "    return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=\"only_first\",  # or \"only_second\"\n",
        "          return_overflowing_tokens=True,\n",
        "          padding=\"max_length\", max_length=128)\n",
        "\n",
        "encoded_emote = yt_emote_dataset.map(tokenize_emote, batched=True)\n",
        "encoded_emote = encoded_emote.rename_column(\"label\", \"labels\")\n",
        "encoded_emote.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# 使用前面 MLM 預訓練完成的模型初始化\n",
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "emote_model = RobertaForSequenceClassification.from_pretrained(\"./roberta_elco_mlm\", num_labels=2)\n",
        "\n",
        "emote_args = TrainingArguments(\n",
        "    output_dir=\"./roberta_emote_finetune_youtube\",  # ✅ output 改個名字\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=5,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=300,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to=\"none\"  # 關掉 wandb\n",
        ")\n",
        "\n",
        "emote_trainer = Trainer(\n",
        "    model=emote_model,\n",
        "    args=emote_args,\n",
        "    train_dataset=encoded_emote,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "emote_trainer.train()\n",
        "emote_model.save_pretrained(\"./roberta_emote_finetune\")\n",
        "tokenizer.save_pretrained(\"./roberta_emote_finetune\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nkkDhO-vyDl"
      },
      "outputs": [],
      "source": [
        "# ========== 4. YouTube 情緒分類 with emoji2vec ==========\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/final_balanced_training_set(1).csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/final_test_set_processed(1).csv')\n",
        "sentiment_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "train_df['Sentiment'] = train_df['Sentiment'].map(sentiment_map)\n",
        "test_df['Sentiment'] = test_df['Sentiment'].map(sentiment_map)\n",
        "\n",
        "class YoutubeDatasetWithEmoji(TorchDataset):\n",
        "    def __init__(self, df, tokenizer, max_length=128):\n",
        "        self.texts = df['CommentText'].tolist()\n",
        "        self.labels = df['Sentiment'].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self): return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors='pt')\n",
        "        item = {k: v.squeeze() for k, v in encoding.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        item['emoji_vec'] = torch.tensor(get_emoji_vector(text), dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "class RobertaWithEmoji(nn.Module):\n",
        "    def __init__(self, model_path, emoji_dim=300, num_labels=3):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(model_path)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.emoji_proj = nn.Linear(emoji_dim, 768)\n",
        "        self.classifier = nn.Linear(768 + 768, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emoji_vec, labels=None):\n",
        "        roberta_out = self.roberta(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "        emoji_embed = self.emoji_proj(emoji_vec)\n",
        "        concat = torch.cat([roberta_out, emoji_embed], dim=1)\n",
        "        logits = self.classifier(self.dropout(concat))\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
        "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
        "        'emoji_vec': torch.stack([b['emoji_vec'] for b in batch]),\n",
        "        'labels': torch.stack([b['labels'] for b in batch])\n",
        "    }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    macro_f1 = f1_score(labels, preds, average='macro')\n",
        "    precision = precision_score(labels, preds, average='macro')\n",
        "    recall = recall_score(labels, preds, average='macro')\n",
        "    try:\n",
        "        y_true_bin = label_binarize(labels, classes=[0,1,2])\n",
        "        auc = roc_auc_score(y_true_bin, logits, multi_class='ovr', average='macro')\n",
        "    except:\n",
        "        auc = 0.0\n",
        "    return {\"accuracy\": acc, \"macro_f1\": macro_f1, \"precision\": precision, \"recall\": recall, \"auc\": auc}\n",
        "\n",
        "model = RobertaWithEmoji(\"./roberta_emote_finetune\")\n",
        "train_dataset = YoutubeDatasetWithEmoji(train_df, tokenizer)\n",
        "test_dataset = YoutubeDatasetWithEmoji(test_df, tokenizer)\n",
        "\n",
        "finetune_args = TrainingArguments(\n",
        "    output_dir=\"./roberta_youtube_emoji_sentiment\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir=\"./logs_youtube\",\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    do_eval=True,\n",
        "    do_train=True,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=300,\n",
        "    lr_scheduler_type=\"linear\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=finetune_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=custom_collate_fn\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(\"\\n🎯 Final Evaluation:\", results)\n",
        "\n",
        "# 準備 Dataloader\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=custom_collate_fn)\n",
        "\n",
        "# 儲存欄位\n",
        "texts = test_df['CommentText'].tolist()\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "emoji_lists = []\n",
        "\n",
        "# 預測 loop\n",
        "for i, batch in enumerate(tqdm(test_loader, desc=\"🔍 Predicting\")):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    emoji_vec = batch['emoji_vec'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, emoji_vec=emoji_vec)\n",
        "        logits = outputs[\"logits\"]\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "    true_labels.extend(labels.cpu().tolist())\n",
        "    pred_labels.extend(preds.cpu().tolist())\n",
        "\n",
        "# 抽取 emoji 函數\n",
        "def extract_emojis(text):\n",
        "    return ' '.join(re.findall(r\"[^\\w\\s,.!?\\'\\\"@#$%^&*()<>+=:;~`]+\", text))\n",
        "\n",
        "emoji_lists = [extract_emojis(t) for t in texts]\n",
        "\n",
        "# 建立 DataFrame 並匯出\n",
        "df_result = pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"pred_label\": pred_labels,\n",
        "    \"emoji_list\": emoji_lists\n",
        "})\n",
        "\n",
        "df_result.to_csv(\"youtube_emoji_sentiment_pretrain.csv\", index=False)\n",
        "print(\"✅ 結果已匯出至 youtube_emoji_sentiment_pretrain.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neXaYus0JZbU"
      },
      "source": [
        "# pretrain (測試的)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "id": "_rY7br_tRZIk",
        "outputId": "cca4783c-5a6b-456f-82fb-98d6f55fd72b"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 934,
          "referenced_widgets": [
            "85b821af1cce46489697a6f9e30872de",
            "9be2dca9fc374707bfc99cb9bc5224f3",
            "12b35716285d4ee89f8c11d677ffec56",
            "db3dd205b59e48c48809bd98f14380ac",
            "d21f0d7e3432467896574ec6e495bb06",
            "70b46f9af0904ac9a495b7dfa01a9db3",
            "0a3c076ab6d946b895b9a6217ae6ee94",
            "8876a9f17359455dbadde11c95ed8ba5",
            "142206ec5e264639b59775580ade032d",
            "612dd1c7c7ad49d08898354126ad412a",
            "6247b1a79df040ab84fb82c320010479",
            "96390c038fb34f24acfb758833760d9f",
            "7238d63e779d4a4385057d0d4d0de6d7",
            "56547feabbc24437ad28a56365a37f4f",
            "be31b85cabc34378a59bc83a3907998f",
            "c7c21bb41830410dba2913f16ab1ac01",
            "483f6c70cfcc4cc59a50b73088d64c20",
            "3763e1efaf244c17aa04d6da51e81606",
            "3118e2b2406d4bb795f6f316abc1c0e3",
            "99ddc9a165df4627bd83f1117921a35a",
            "6dfb001001944daca49d4220dcc6f27d",
            "36d83aeaf62d435d845900117986b0d8"
          ]
        },
        "id": "FTrbY1WxJZbV",
        "outputId": "6decc9a3-6b0f-4465-dfb1-eb65fa94726f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling,\n",
        "    TrainingArguments, Trainer\n",
        ")\n",
        "\n",
        "# ========== 1. 載入 ELCo 資料 ==========\n",
        "elco_df = pd.read_csv(\"/content/ELCo/ELCo.csv\")\n",
        "\n",
        "# 建立多樣化 MLM 樣式句子\n",
        "templates = [\n",
        "    lambda em, en: f\"{em} usually means {en}.\",\n",
        "    lambda em, en: f\"When people use {em}, they are likely feeling {en}.\",\n",
        "    lambda em, en: f\"{em} is often used to show {en}.\",\n",
        "    lambda em, en: f\"Using {em} might suggest that someone is experiencing {en}.\",\n",
        "    lambda em, en: f\"The emoji {em} stands for {en} in many cases.\",\n",
        "]\n",
        "elco_df['text'] = [random.choice(templates)(em, en) for em, en in zip(elco_df[\"EM\"], elco_df[\"EN\"])]\n",
        "\n",
        "# ========== 2. 切分 train / val ==========\n",
        "train_texts, val_texts = train_test_split(elco_df['text'], test_size=0.1, random_state=42)\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': train_texts}))\n",
        "val_dataset = Dataset.from_pandas(pd.DataFrame({'text': val_texts}))\n",
        "\n",
        "# ========== 3. Tokenizer ==========\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "def tokenize_mlm(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_mlm, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_mlm, batched=True)\n",
        "\n",
        "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "# ========== 4. 訓練模型 ==========\n",
        "mlm_model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "mlm_args = TrainingArguments(\n",
        "    output_dir=\"./roberta_elco_mlm\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    learning_rate=1e-4,\n",
        "    eval_strategy=\"epoch\",  # ✅ 每個 epoch 都 evaluate\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "mlm_trainer = Trainer(\n",
        "    model=mlm_model,\n",
        "    args=mlm_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "mlm_trainer.train()\n",
        "\n",
        "# ========== 5. Evaluate：計算 Perplexity ==========\n",
        "eval_results = mlm_trainer.evaluate()\n",
        "perplexity = math.exp(eval_results[\"eval_loss\"])\n",
        "print(f\"🔍 Evaluation Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "# ========== 6. 儲存模型 ==========\n",
        "mlm_model.save_pretrained(\"./roberta_elco_mlm\")\n",
        "tokenizer.save_pretrained(\"./roberta_elco_mlm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 1000,
          "referenced_widgets": [
            "0404219ade98476791e0a7d0ee83406f",
            "d3415a2500ca4f89a8fcb99cd422eef7",
            "b5ae3c761e2e4537ba96b05a8aa9c924",
            "4c18e06aa4a94d5d8564cc7aea7a9dfc",
            "589c7b43cd4d498198e6c28ad17eae36",
            "7b4fc66140c94712aebf498e754dbd30",
            "a09f28eaded24f4f842c2e2189da64a9",
            "a20573652d5340ddbee9781c3b12271f",
            "f33244179ddc4693803133bb500a199b",
            "08729ff19cf64429b42d64bb12ed243f",
            "4180d7e6eefc426f863df4cf0ff552d7",
            "376f015ef3ac4780a9e78635657415bd",
            "4a0eb8333c3f4f48b8836821011ee0d4",
            "eddaafe0aa484c25a9e2f66c9ef106cf",
            "b208c4a4b0e542e78aeba73a6512ff91",
            "d00e008776fe4c4f8108711a5e950bb7",
            "98210b4cf49d43d58742b57d7ac18cd0",
            "9347169d1cd748e1b698bc7fd226fec3",
            "7ffcbb0c01df4c2aabd4dfc2ba0350ea",
            "26c203d718f147f8879a0d441075a4d0",
            "22aaf78e1a444b6e8d7de0857ef5b2a8",
            "43bcb3bf17314ce487f97b4f80e4a891"
          ]
        },
        "id": "h4_jlO8uJZbV",
        "outputId": "753bc227-a1ee-4e44-ff14-2fdc38dc8e40"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    RobertaTokenizer, RobertaForSequenceClassification,\n",
        "    TrainingArguments, Trainer, EvalPrediction\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# ========== 1. 讀入資料集 ==========\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/____YouTube_Emote____.csv\")\n",
        "df = df.dropna(subset=[\"premise\", \"hypothesis\", \"label\"])  # 確保無缺漏資料\n",
        "\n",
        "# 切分訓練 / 驗證集\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"label\"])\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# ========== 2. Tokenizer ==========\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"./roberta_elco_mlm\")\n",
        "\n",
        "def tokenize_emote(example):\n",
        "    return tokenizer(example[\"premise\"], example[\"hypothesis\"],\n",
        "                     truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_emote, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_emote, batched=True)\n",
        "\n",
        "for dataset in [tokenized_train, tokenized_val]:\n",
        "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
        "    dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# ========== 3. 載入 MLM 預訓練模型並 fine-tune ==========\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"./roberta_elco_mlm\", num_labels=2)\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds),\n",
        "        \"recall\": recall_score(labels, preds)\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./roberta_emote_finetune_youtube\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=5,\n",
        "    per_device_eval_batch_size=16,\n",
        "    eval_strategy=\"epoch\",  # ✅ 每個 epoch 都 evaluate\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=300,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ========== 4. 訓練與評估 ==========\n",
        "trainer.train()\n",
        "eval_result = trainer.evaluate()\n",
        "print(\"📊 Evaluation Results:\", eval_result)\n",
        "\n",
        "# ========== 5. 儲存模型 ==========\n",
        "model.save_pretrained(\"./roberta_emote_finetune\")\n",
        "tokenizer.save_pretrained(\"./roberta_emote_finetune\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 464
        },
        "id": "ePl3UjFcJZbW",
        "outputId": "c5fa1c37-7474-4a34-a5b7-9a03a10e567f"
      },
      "outputs": [],
      "source": [
        "# ========== 4. YouTube 情緒分類 with emoji2vec ==========\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/final_balanced_training_set(1).csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/final_test_set_processed(1).csv')\n",
        "sentiment_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "train_df['Sentiment'] = train_df['Sentiment'].map(sentiment_map)\n",
        "test_df['Sentiment'] = test_df['Sentiment'].map(sentiment_map)\n",
        "\n",
        "class YoutubeDatasetWithEmoji(TorchDataset):\n",
        "    def __init__(self, df, tokenizer, max_length=128):\n",
        "        self.texts = df['CommentText'].tolist()\n",
        "        self.labels = df['Sentiment'].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self): return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors='pt')\n",
        "        item = {k: v.squeeze() for k, v in encoding.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        item['emoji_vec'] = torch.tensor(get_emoji_vector(text), dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "class RobertaWithEmoji(nn.Module):\n",
        "    def __init__(self, model_path, emoji_dim=300, num_labels=3):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(model_path)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.emoji_proj = nn.Linear(emoji_dim, 768)\n",
        "        self.classifier = nn.Linear(768 + 768, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emoji_vec, labels=None):\n",
        "        roberta_out = self.roberta(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "        emoji_embed = self.emoji_proj(emoji_vec)\n",
        "        concat = torch.cat([roberta_out, emoji_embed], dim=1)\n",
        "        logits = self.classifier(self.dropout(concat))\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
        "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
        "        'emoji_vec': torch.stack([b['emoji_vec'] for b in batch]),\n",
        "        'labels': torch.stack([b['labels'] for b in batch])\n",
        "    }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    macro_f1 = f1_score(labels, preds, average='macro')\n",
        "    precision = precision_score(labels, preds, average='macro')\n",
        "    recall = recall_score(labels, preds, average='macro')\n",
        "    try:\n",
        "        y_true_bin = label_binarize(labels, classes=[0,1,2])\n",
        "        auc = roc_auc_score(y_true_bin, logits, multi_class='ovr', average='macro')\n",
        "    except:\n",
        "        auc = 0.0\n",
        "    return {\"accuracy\": acc, \"macro_f1\": macro_f1, \"precision\": precision, \"recall\": recall, \"auc\": auc}\n",
        "\n",
        "model = DebertaWithEmoji(\"./roberta_emote_finetune\")\n",
        "train_dataset = YoutubeDatasetWithEmoji(train_df, tokenizer)\n",
        "test_dataset = YoutubeDatasetWithEmoji(test_df, tokenizer)\n",
        "\n",
        "finetune_args = TrainingArguments(\n",
        "    output_dir=\"./roberta_youtube_emoji_sentiment\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir=\"./logs_youtube\",\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    do_eval=True,\n",
        "    do_train=True,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=300,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    eval_strategy=\"epoch\",  # ✅ 每個 epoch 都 evaluate\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=finetune_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=custom_collate_fn\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(\"\\n🎯 Final Evaluation:\", results)\n",
        "\n",
        "# 準備 Dataloader\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=custom_collate_fn)\n",
        "\n",
        "# 儲存欄位\n",
        "texts = test_df['CommentText'].tolist()\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "emoji_lists = []\n",
        "\n",
        "# 預測 loop\n",
        "for i, batch in enumerate(tqdm(test_loader, desc=\"🔍 Predicting\")):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    emoji_vec = batch['emoji_vec'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, emoji_vec=emoji_vec)\n",
        "        logits = outputs[\"logits\"]\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "    true_labels.extend(labels.cpu().tolist())\n",
        "    pred_labels.extend(preds.cpu().tolist())\n",
        "\n",
        "# 抽取 emoji 函數\n",
        "def extract_emojis(text):\n",
        "    return ' '.join(re.findall(r\"[^\\w\\s,.!?\\'\\\"@#$%^&*()<>+=:;~`]+\", text))\n",
        "\n",
        "emoji_lists = [extract_emojis(t) for t in texts]\n",
        "\n",
        "# 建立 DataFrame 並匯出\n",
        "df_result = pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"pred_label\": pred_labels,\n",
        "    \"emoji_list\": emoji_lists\n",
        "})\n",
        "\n",
        "df_result.to_csv(\"youtube_emoji_sentiment_pretrain.csv\", index=False)\n",
        "print(\"✅ 結果已匯出至 youtube_emoji_sentiment_pretrain.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY2dKRAj25WB"
      },
      "source": [
        "# pretrain (deberta)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "id": "hR67qtJ725WB",
        "outputId": "cca4783c-5a6b-456f-82fb-98d6f55fd72b"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 1000,
          "referenced_widgets": [
            "1db4ba3699b94aafbe55a3576d7ccaf6",
            "39d36203e3ef4a83b4aa9f1b557bd8dc",
            "e15d65d2cdee42d596b17f0b2596ad0d",
            "6660212c23224665bd8221df9d9d77a1",
            "757c9d70663748d7b1c7552cd9dcc829",
            "55d7d194a87e45b2a9a5f568a856565f",
            "f9761a67b1054c688c4bf47c3a8921f2",
            "5eb8ba8a53e44f6c93a6252beb199063",
            "b99034119c6c4b4f95e9de6e0ed8ba70",
            "398fb41693fd4061b314aec49552624d",
            "93db4ba108d14c8197e52d64850e224d",
            "1e5e56e9535d468caa8893b7eff2f0f2",
            "5fdb7f9242d340308c6759772f3c13a4",
            "7a293c9dfb314a24a850fff2895fd61f",
            "3a4d7d9921cc41178a42430018186443",
            "3afab69bff1b4d16a7a4d05c2b3ac8ab",
            "ba8ae03cc93e40a0a5a83b4ec9d4d676",
            "9636a96b6b804ff395b1e4a23933be56",
            "0650dc28224e403db79161607114bf43",
            "35e5d4fb0fd842a691c832f521d3f446",
            "717457b36a7d4d079c13d9e72e84596f",
            "dbd0e2fd8d534fdaa8e32a7cdaeb7d04",
            "619a2ee4150a4bcd833cd263bf81723d",
            "15556be4139c40e8a4b8866662551c89",
            "f9c358f8d51140e08145b1efe5474312",
            "1223dd4f3315446a898f3f5d1ff43d3f",
            "d4e7552fdb35442a8cefbc0cbf9e8d1d",
            "fe8b664ba4c644228d9736991d9bffd1",
            "e6e2a116d949454dbe76b93c5fa61790",
            "1a2117c52e894d8987e9405241cd49c6",
            "8c82aff0eca64edc89e149d42709f812",
            "424a9a266cf54513a77bb69414338904",
            "b8427d3111034af3a8c65ed801f46588",
            "2c7811222693428a97b17f66cc8a32a4",
            "e8ab5823d30c46d09729abdda3227ca1",
            "4a69ab47437342eb8b32b5805d08a700",
            "abfdd5e43d7d48fcb5f32aceb1301273",
            "31b60d66ed16472cb9d3f3c1392ff8d6",
            "d10e8151e11848b396b70ed8c64875ae",
            "bc347795dea043fc816bccc45381ffbe",
            "e3b2a4799164497c94e61118d74c8214",
            "3bce3aad33e54baeac935ca40529d3bb",
            "f1c68797d7014de1b0428da2f0fbdc1f",
            "d2d13bc1ebbb4fb5857c66398812b273"
          ]
        },
        "id": "ZG6XmfcU25WC",
        "outputId": "4857b5a2-4436-4ed8-c1a8-21bb4e83c5b5"
      },
      "outputs": [],
      "source": [
        "# ✅ 完整整合流程：使用 DeBERTa + emoji2vec，無位置特徵\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    DebertaTokenizer, DebertaForMaskedLM, DebertaModel,\n",
        "    DebertaForSequenceClassification,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments, Trainer, EvalPrediction\n",
        ")\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from gensim.models import KeyedVectors\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# ========== STEP 1: MLM 預訓練 ==========\n",
        "elco_df = pd.read_csv(\"/content/ELCo/ELCo.csv\")\n",
        "templates = [\n",
        "    lambda em, en: f\"{em} usually means {en}.\",\n",
        "    lambda em, en: f\"When people use {em}, they are likely feeling {en}.\",\n",
        "    lambda em, en: f\"{em} is often used to show {en}.\",\n",
        "    lambda em, en: f\"Using {em} might suggest that someone is experiencing {en}.\",\n",
        "    lambda em, en: f\"The emoji {em} stands for {en} in many cases.\"\n",
        "]\n",
        "elco_df['text'] = [random.choice(templates)(em, en) for em, en in zip(elco_df[\"EM\"], elco_df[\"EN\"])]\n",
        "\n",
        "train_texts, val_texts = train_test_split(elco_df['text'], test_size=0.1, random_state=42)\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': train_texts}))\n",
        "val_dataset = Dataset.from_pandas(pd.DataFrame({'text': val_texts}))\n",
        "\n",
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "mlm_model = DebertaForMaskedLM.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "def tokenize_mlm(example):\n",
        "    return tokenizer(example['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_mlm, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_mlm, batched=True)\n",
        "\n",
        "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "mlm_args = TrainingArguments(\n",
        "    output_dir=\"./deberta_elco_mlm\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    learning_rate=1e-4,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "mlm_trainer = Trainer(\n",
        "    model=mlm_model,\n",
        "    args=mlm_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "mlm_trainer.train()\n",
        "mlm_model.save_pretrained(\"./deberta_elco_mlm\")\n",
        "tokenizer.save_pretrained(\"./deberta_elco_mlm\")\n",
        "\n",
        "\n",
        "# step2\n",
        "# ========== 1. 讀入資料集 ==========\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/____YouTube_Emote____.csv\")\n",
        "df = df.dropna(subset=[\"premise\", \"hypothesis\", \"label\"])  # 確保無缺漏資料\n",
        "\n",
        "# 切分訓練 / 驗證集\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"label\"])\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# ========== 2. Tokenizer ==========\n",
        "tokenizer = DebertaTokenizer.from_pretrained(\"./deberta_elco_mlm\")\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_emote(example):\n",
        "    return tokenizer(example[\"premise\"], example[\"hypothesis\"],\n",
        "                     truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_emote, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_emote, batched=True)\n",
        "\n",
        "for dataset in [tokenized_train, tokenized_val]:\n",
        "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
        "    dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# ========== 3. 載入 MLM 預訓練模型並 fine-tune ==========\n",
        "model = DebertaForSequenceClassification.from_pretrained(\"./deberta_elco_mlm\", num_labels=2)\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds),\n",
        "        \"recall\": recall_score(labels, preds)\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./roberta_emote_finetune_youtube\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=5,\n",
        "    per_device_eval_batch_size=16,\n",
        "    eval_strategy=\"epoch\",  # ✅ 每個 epoch 都 evaluate\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=300,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ========== 4. 訓練與評估 ==========\n",
        "trainer.train()\n",
        "eval_result = trainer.evaluate()\n",
        "print(\"📊 Evaluation Results:\", eval_result)\n",
        "\n",
        "# ========== 5. 儲存模型 ==========\n",
        "model.save_pretrained(\"./deberta_emote_finetune\")\n",
        "tokenizer.save_pretrained(\"./deberta_emote_finetune\")\n",
        "# ========== STEP 2: 情緒分類（含 emoji2vec） ==========\n",
        "emoji_model = KeyedVectors.load_word2vec_format(\"/content/emoji2vec/pre-trained/emoji2vec.bin\", binary=True)\n",
        "\n",
        "def get_emoji_vector(text):\n",
        "    emojis = re.findall(r\"[\\U00010000-\\U0010ffff]\", text)\n",
        "    if not emojis: return np.zeros(300)\n",
        "    vecs = [emoji_model[e] for e in emojis if e in emoji_model]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(300)\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/final_balanced_training_set(1).csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/final_test_set_processed(1).csv')\n",
        "sentiment_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "train_df['Sentiment'] = train_df['Sentiment'].map(sentiment_map)\n",
        "test_df['Sentiment'] = test_df['Sentiment'].map(sentiment_map)\n",
        "\n",
        "class YoutubeDatasetWithEmoji(TorchDataset):\n",
        "    def __init__(self, df, tokenizer, max_length=128):\n",
        "        self.texts = df['CommentText'].tolist()\n",
        "        self.labels = df['Sentiment'].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self): return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors='pt')\n",
        "        item = {k: v.squeeze() for k, v in encoding.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        item['emoji_vec'] = torch.tensor(get_emoji_vector(text), dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "class DebertaWithEmoji(nn.Module):\n",
        "    def __init__(self, model_path, emoji_dim=300, num_labels=3):\n",
        "        super().__init__()\n",
        "        self.deberta = DebertaModel.from_pretrained(model_path)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.emoji_proj = nn.Linear(emoji_dim, 768)\n",
        "        self.classifier = nn.Linear(768 + 768, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emoji_vec, labels=None):\n",
        "        deberta_out = self.deberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
        "        emoji_embed = self.emoji_proj(emoji_vec)\n",
        "        concat = torch.cat([deberta_out, emoji_embed], dim=1)\n",
        "        logits = self.classifier(self.dropout(concat))\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
        "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
        "        'emoji_vec': torch.stack([b['emoji_vec'] for b in batch]),\n",
        "        'labels': torch.stack([b['labels'] for b in batch])\n",
        "    }\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": f1_score(labels, preds, average='macro'),\n",
        "        \"precision\": precision_score(labels, preds, average='macro'),\n",
        "        \"recall\": recall_score(labels, preds, average='macro'),\n",
        "        \"auc\": roc_auc_score(label_binarize(labels, classes=[0, 1, 2]), logits, average='macro', multi_class='ovr')\n",
        "    }\n",
        "\n",
        "# ========== Training ==========\n",
        "model = DebertaWithEmoji(\"./deberta_emote_finetune\")\n",
        "tokenizer = DebertaTokenizer.from_pretrained(\"./deberta_emote_finetune\")\n",
        "train_dataset = YoutubeDatasetWithEmoji(train_df, tokenizer)\n",
        "test_dataset = YoutubeDatasetWithEmoji(test_df, tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./deberta_youtube_emoji_sentiment\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        learning_rate=2e-5,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        logging_steps=50,\n",
        "    ),\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"✅ 評估結果：\", trainer.evaluate())\n",
        "\n",
        "# ========== 預測輸出 ==========\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "texts = test_df['CommentText'].tolist()\n",
        "true_labels, pred_labels = [], []\n",
        "\n",
        "for batch in tqdm(loader, desc=\"🔍 Predicting\"):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    emoji_vec = batch['emoji_vec'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, emoji_vec=emoji_vec)\n",
        "        preds = torch.argmax(outputs[\"logits\"], dim=1)\n",
        "\n",
        "    true_labels.extend(labels.cpu().tolist())\n",
        "    pred_labels.extend(preds.cpu().tolist())\n",
        "\n",
        "emoji_lists = [' '.join(re.findall(r\"[^\\w\\s,.!?\\'\\\"@#$%^&*()<>+=:;~`]+\", t)) for t in texts]\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"pred_label\": pred_labels,\n",
        "    \"emoji_list\": emoji_lists\n",
        "}).to_csv(\"youtube_emoji_sentiment_pretrain.csv\", index=False)\n",
        "print(\"✅ 匯出完成：youtube_emoji_sentiment_pretrain.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 466
        },
        "id": "cCL_5XLoA4S0",
        "outputId": "ef401a01-b6ad-47cd-cbd5-132ecbf63855"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== STEP 3: YouTube 情緒分析 with emoji features ======\n",
        "def get_emoji_vec(text):\n",
        "    emojis = re.findall(r\"[\\U00010000-\\U0010ffff]\", text)\n",
        "    if not emojis: return np.zeros(300)\n",
        "    vecs = [emoji_model[e] for e in emojis if e in emoji_model]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(300)\n",
        "\n",
        "def get_emoji_position(text):\n",
        "    emojis = re.findall(r\"[\\U00010000-\\U0010ffff]\", text)\n",
        "    if not emojis: return 0\n",
        "    emoji_str = ''.join(emojis)\n",
        "    idx = text.find(emoji_str)\n",
        "    return 1 if idx / max(1, len(text)) > 0.5 else 0\n",
        "\n",
        "emoji_model = KeyedVectors.load_word2vec_format(\"/content/emoji2vec/pre-trained/emoji2vec.bin\", binary=True)\n",
        "yt_train = pd.read_csv('/content/drive/MyDrive/final_balanced_training_set(1).csv')\n",
        "yt_test = pd.read_csv('/content/drive/MyDrive/final_test_set_processed(1).csv')\n",
        "label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "yt_train[\"Sentiment\"] = yt_train[\"Sentiment\"].map(label_map)\n",
        "yt_test[\"Sentiment\"] = yt_test[\"Sentiment\"].map(label_map)\n",
        "\n",
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "# 移除 position 特徵，只保留 emoji_vec\n",
        "def get_emoji_vec(text):\n",
        "    emojis = re.findall(r\"[\\U00010000-\\U0010ffff]\", text)\n",
        "    if not emojis: return np.zeros(300)\n",
        "    vecs = [emoji_model[e] for e in emojis if e in emoji_model]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(300)\n",
        "\n",
        "# 刪除 position 特徵\n",
        "class YouTubeEmojiDataset(TorchDataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = row['CommentText']\n",
        "        label = row['Sentiment']\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
        "        emoji_vec = get_emoji_vec(text)\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long),\n",
        "            'emoji_vec': torch.tensor(emoji_vec, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# 刪除 position 特徵嵌入層\n",
        "class DebertaWithEmoji(nn.Module):\n",
        "    def __init__(self, model_path, emoji_dim=300, num_labels=3):\n",
        "        super().__init__()\n",
        "        self.deberta = DebertaModel.from_pretrained(model_path)\n",
        "        self.emoji_proj = nn.Linear(emoji_dim, 768)\n",
        "        self.classifier = nn.Linear(768 + 768, num_labels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emoji_vec, labels=None):\n",
        "        deberta_out = self.deberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
        "        emoji_embed = self.emoji_proj(emoji_vec)\n",
        "        combined = torch.cat([deberta_out, emoji_embed], dim=1)\n",
        "        logits = self.classifier(self.dropout(combined))\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "# 修改 collate_fn\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
        "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
        "        'emoji_vec': torch.stack([b['emoji_vec'] for b in batch]),\n",
        "        'labels': torch.stack([b['labels'] for b in batch])\n",
        "    }\n",
        "\n",
        "# 評估與訓練流程保持不變\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": f1_score(labels, preds, average='macro'),\n",
        "        \"precision\": precision_score(labels, preds, average='macro'),\n",
        "        \"recall\": recall_score(labels, preds, average='macro'),\n",
        "        \"auc\": roc_auc_score(label_binarize(labels, classes=[0,1,2]), logits, average='macro', multi_class='ovr')\n",
        "    }\n",
        "\n",
        "train_set = YouTubeEmojiDataset(yt_train, tokenizer)\n",
        "test_set = YouTubeEmojiDataset(yt_test, tokenizer)\n",
        "model = DebertaWithEmoji(\"./deberta_emote_finetune\")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./deberta_youtube_sentiment\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=test_set,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "eval_result = trainer.evaluate()\n",
        "print(\"Evaluation:\", eval_result)\n",
        "\n",
        "# ========== 預測並匯出結果 ==========\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# 切換到評估模式\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 準備 Dataloader\n",
        "test_loader = DataLoader(test_set, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "# 儲存欄位\n",
        "texts = yt_test['CommentText'].tolist()\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "emoji_lists = []\n",
        "\n",
        "# 預測 loop\n",
        "for i, batch in enumerate(tqdm(test_loader, desc=\"🔍 Predicting\")):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    emoji_vec = batch['emoji_vec'].to(device)\n",
        "    emoji_pos = batch['emoji_pos'].to(device) # Add this line to get emoji_pos from the batch\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Pass emoji_pos to the model's forward method\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, emoji_vec=emoji_vec, emoji_pos=emoji_pos)\n",
        "        logits = outputs[\"logits\"]\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "    true_labels.extend(labels.cpu().tolist())\n",
        "    pred_labels.extend(preds.cpu().tolist())\n",
        "\n",
        "# 抽取 emoji 函數\n",
        "def extract_emojis(text):\n",
        "    return ' '.join(re.findall(r\"[^\\w\\s,.!?\\'\\\"@#$%^&*()<>+=:;~`]+\", text))\n",
        "\n",
        "emoji_lists = [extract_emojis(t) for t in texts]\n",
        "\n",
        "# 建立 DataFrame 並匯出\n",
        "df_result = pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"pred_label\": pred_labels,\n",
        "    \"emoji_list\": emoji_lists\n",
        "})\n",
        "\n",
        "df_result.to_csv(\"youtube_emoji_sentiment_pretrain+position.csv\", index=False)\n",
        "print(\"✅ 匯出完成：youtube_emoji_sentiment_pretrain+position.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cp6tO_SGEmc"
      },
      "source": [
        "# pretrain (bert)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "id": "kn5cwghvGEmd",
        "outputId": "cca4783c-5a6b-456f-82fb-98d6f55fd72b"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 1000,
          "referenced_widgets": [
            "48b13db32a084b7f938c3134fc84b6bd",
            "a6d1be2db9f24f2a8d80da0aafc58e8b",
            "4086868194a0406d9debc755e3cd16da",
            "e064d809a2dc4cd8943ad33bbce88a88",
            "d81c5aa8f1de45b6814e5b87fff9d169",
            "2fafce65dc5a4f0da24d8fe89ba01253",
            "11b6a9b58a2541b39b6a88700c209436",
            "2d308f467cea4d6789e416e3834b161c",
            "b2787a9fc1c744dfbdb620cb6252b547",
            "125e8ea499364e689dcf6cffa09619b4",
            "043a0aac606d4419b794ce189bdaa23c",
            "d302294945df478db7552c2adc8c9483",
            "063ebabe7b904d35be8af23b20b1a414",
            "de27333b1175433cbb8d4591927c6b73",
            "a1471aed352c42259def859ee39687f4",
            "adb40aca489a4d2b9bb1c2d3ca8aa1f5",
            "4e2a28590da74aa19c9a679cccd4a849",
            "6a6bac43f3e04351a535a71ebb259c4e",
            "dad41828aa5d4a1aa14bf42a4298cfd2",
            "28ab37d1931e42aa9b36c67f32a48356",
            "9e0cbe9d02964f57bfccc20e9ebfe2c4",
            "1540735707f440bf9386f19e400ab70c",
            "8ae381e76ad0462e9dffd1025507581d",
            "584a0a46d73a478e8cab1530dde8c978",
            "5318f01fd4af41489e458a71009da065",
            "c85a779bcd4c424dace2d125b6024189",
            "a7953ceb71ed491683725469a169cc27",
            "d7041e31c26446a49c36154b51b17a63",
            "79e4f2f4cf5848219a599a479b4ce40f",
            "d0690f5533f6458e94877e7f9bd71bd7",
            "5ab2ad7e2b864c049c48f8f412234cbe",
            "8bf0e92fa2c64afaad055c70cd4d5e68",
            "da5bbfa666b746f5b4627b3ec9c6b26b",
            "0e8aefcf6338400f9ec7e3f12cef3212",
            "d8728198b1f84394945f322dc4f0c973",
            "b3409bad43d04450b9363d94e75625c4",
            "3685901f578643c4b99f7d6b3a744be1",
            "7d824ea4b30746019ed526454ac01510",
            "ba401ace8df745e3b634ede16fe1473d",
            "5dfd7eb7c19b4cff8f7f0c8280f24695",
            "3f854556266e47adaad15704c273de8c",
            "7f7e53d4bda3465186147787801cd98c",
            "f4adf412a5404f33b5b4a439b9f5b7d0",
            "b8ea4017354741debd81b65a9bde51e6"
          ]
        },
        "id": "4ztJev_0GJEJ",
        "outputId": "f756da84-57dd-485b-ac95-923253173760"
      },
      "outputs": [],
      "source": [
        "# ✅ 完整整合流程：使用 BERT + emoji2vec，無位置特徵\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    BertTokenizer, BertForMaskedLM, BertModel,\n",
        "    BertForSequenceClassification,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments, Trainer, EvalPrediction\n",
        ")\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from gensim.models import KeyedVectors\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# ========== STEP 1: MLM 預訓練 ==========\n",
        "elco_df = pd.read_csv(\"/content/ELCo/ELCo.csv\")\n",
        "templates = [\n",
        "    lambda em, en: f\"{em} usually means {en}.\",\n",
        "    lambda em, en: f\"When people use {em}, they are likely feeling {en}.\",\n",
        "    lambda em, en: f\"{em} is often used to show {en}.\",\n",
        "    lambda em, en: f\"Using {em} might suggest that someone is experiencing {en}.\",\n",
        "    lambda em, en: f\"The emoji {em} stands for {en} in many cases.\"\n",
        "]\n",
        "elco_df['text'] = [random.choice(templates)(em, en) for em, en in zip(elco_df[\"EM\"], elco_df[\"EN\"])]\n",
        "\n",
        "train_texts, val_texts = train_test_split(elco_df['text'], test_size=0.1, random_state=42)\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': train_texts}))\n",
        "val_dataset = Dataset.from_pandas(pd.DataFrame({'text': val_texts}))\n",
        "\n",
        "mlm_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "mlm_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_mlm(example):\n",
        "    return mlm_tokenizer(example['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_mlm, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_mlm, batched=True)\n",
        "\n",
        "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=mlm_tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "mlm_args = TrainingArguments(\n",
        "    output_dir=\"./bert_elco_mlm\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    learning_rate=1e-4,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "mlm_trainer = Trainer(\n",
        "    model=mlm_model,\n",
        "    args=mlm_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "mlm_trainer.train()\n",
        "mlm_model.save_pretrained(\"./bert_elco_mlm\")\n",
        "mlm_tokenizer.save_pretrained(\"./bert_elco_mlm\")\n",
        "\n",
        "# ========== 1. 讀入資料集 ==========\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/____YouTube_Emote____.csv\")\n",
        "df = df.dropna(subset=[\"premise\", \"hypothesis\", \"label\"])  # 確保無缺漏資料\n",
        "\n",
        "# 切分訓練 / 驗證集\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"label\"])\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# ========== 2. Tokenizer ==========\n",
        "tokenizer = BertTokenizer.from_pretrained(\"./bert_elco_mlm\")\n",
        "\n",
        "\n",
        "def tokenize_emote(example):\n",
        "    return tokenizer(example[\"premise\"], example[\"hypothesis\"],\n",
        "                     truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_emote, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_emote, batched=True)\n",
        "\n",
        "for dataset in [tokenized_train, tokenized_val]:\n",
        "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
        "    dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# ========== 3. 載入 MLM 預訓練模型並 fine-tune ==========\n",
        "model = BertForSequenceClassification.from_pretrained(\"./bert_elco_mlm\", num_labels=2)\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds),\n",
        "        \"recall\": recall_score(labels, preds)\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./roberta_emote_finetune_youtube\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=5,\n",
        "    per_device_eval_batch_size=16,\n",
        "    eval_strategy=\"epoch\",  # ✅ 每個 epoch 都 evaluate\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=300,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ========== 4. 訓練與評估 ==========\n",
        "trainer.train()\n",
        "eval_result = trainer.evaluate()\n",
        "print(\"📊 Evaluation Results:\", eval_result)\n",
        "\n",
        "# ========== 5. 儲存模型 ==========\n",
        "model.save_pretrained(\"./bert_emote_finetune\")\n",
        "tokenizer.save_pretrained(\"./bert_emote_finetune\")\n",
        "\n",
        "# ========== STEP 2: 情緒分類（含 emoji2vec） ==========\n",
        "emoji_model = KeyedVectors.load_word2vec_format(\"/content/emoji2vec/pre-trained/emoji2vec.bin\", binary=True)\n",
        "\n",
        "def get_emoji_vector(text):\n",
        "    emojis = re.findall(r\"[\\U00010000-\\U0010ffff]\", text)\n",
        "    if not emojis: return np.zeros(300)\n",
        "    vecs = [emoji_model[e] for e in emojis if e in emoji_model]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(300)\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/final_balanced_training_set(1).csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/final_test_set_processed(1).csv')\n",
        "sentiment_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "train_df['Sentiment'] = train_df['Sentiment'].map(sentiment_map)\n",
        "test_df['Sentiment'] = test_df['Sentiment'].map(sentiment_map)\n",
        "\n",
        "class YoutubeDatasetWithEmoji(TorchDataset):\n",
        "    def __init__(self, df, tokenizer, max_length=128):\n",
        "        self.texts = df['CommentText'].tolist()\n",
        "        self.labels = df['Sentiment'].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self): return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors='pt')\n",
        "        item = {k: v.squeeze() for k, v in encoding.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        item['emoji_vec'] = torch.tensor(get_emoji_vector(text), dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "class BertWithEmoji(nn.Module):\n",
        "    def __init__(self, model_path, emoji_dim=300, num_labels=3):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_path)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.emoji_proj = nn.Linear(emoji_dim, 768)\n",
        "        self.classifier = nn.Linear(768 + 768, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emoji_vec, labels=None):\n",
        "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "        emoji_embed = self.emoji_proj(emoji_vec)\n",
        "        concat = torch.cat([bert_out, emoji_embed], dim=1)\n",
        "        logits = self.classifier(self.dropout(concat))\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
        "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
        "        'emoji_vec': torch.stack([b['emoji_vec'] for b in batch]),\n",
        "        'labels': torch.stack([b['labels'] for b in batch])\n",
        "    }\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": f1_score(labels, preds, average='macro'),\n",
        "        \"precision\": precision_score(labels, preds, average='macro'),\n",
        "        \"recall\": recall_score(labels, preds, average='macro'),\n",
        "        \"auc\": roc_auc_score(label_binarize(labels, classes=[0, 1, 2]), logits, average='macro', multi_class='ovr')\n",
        "    }\n",
        "\n",
        "# ========== Training ==========\n",
        "tokenizer = BertTokenizer.from_pretrained(\"./bert_emote_finetune\")\n",
        "model = BertWithEmoji(\"./bert_emote_finetune\")\n",
        "train_dataset = YoutubeDatasetWithEmoji(train_df, tokenizer)\n",
        "test_dataset = YoutubeDatasetWithEmoji(test_df, tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./bert_youtube_emoji_sentiment\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        learning_rate=2e-5,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        logging_steps=50,\n",
        "    ),\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"✅ 評估結果：\", trainer.evaluate())\n",
        "\n",
        "# ========== 預測輸出 ==========\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "texts = test_df['CommentText'].tolist()\n",
        "true_labels, pred_labels = [], []\n",
        "\n",
        "for batch in tqdm(loader, desc=\"🔍 Predicting\"):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    emoji_vec = batch['emoji_vec'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, emoji_vec=emoji_vec)\n",
        "        preds = torch.argmax(outputs[\"logits\"], dim=1)\n",
        "\n",
        "    true_labels.extend(labels.cpu().tolist())\n",
        "    pred_labels.extend(preds.cpu().tolist())\n",
        "\n",
        "emoji_lists = [' '.join(re.findall(r\"[^\\w\\s,.!?\\'\\\"@#$%^&*()<>+=:;~`]+\", t)) for t in texts]\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"pred_label\": pred_labels,\n",
        "    \"emoji_list\": emoji_lists\n",
        "}).to_csv(\"youtube_emoji_sentiment_pretrain.csv\", index=False)\n",
        "print(\"✅ 匯出完成：youtube_emoji_sentiment_pretrain.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUB-H99iLOMi"
      },
      "source": [
        "# pretrain+position\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 1000,
          "referenced_widgets": [
            "74aa42b17acc40e18b3bb3d8216bf998",
            "4a3210290d5448d1af7543e0bbde3264",
            "abe08e28b62244bf9f05777ed9cbbbe6",
            "619a391c235f453fbb1756642a4f9c7f",
            "aef95cf7cd314024bace8ad51bf6b942",
            "582d28f51c484476969757044b94c873",
            "6821f7cca7fd4b8c802f20f3f5a9f0c8",
            "a5f52caa9d454cd5a5cc14210ffe659b",
            "016b37427b7847078bda5df9b12bd077",
            "0204487569d74ecda08fb7c21a12f542",
            "5fb346de888c4f6ea8bba5b5c2ec9d25",
            "3c45b00921e84d1e92d1fa10c37611f5",
            "62f806cc078043d5bd3b1aae4b99cb8f",
            "f5b4852548aa41b688e790166a06c839",
            "101ecfc44ea44e6e9dc3b7c31339e110",
            "59feba973b9a4b388fc7f1d54c23b954",
            "06c520965aaa46a195824ef010035a9a",
            "be006ceb8eff4dd2acc344d2d487fb19",
            "0508725f644e434bbf6fc84753308fb8",
            "4f41e65eea144e7a94ec62d6b9be9c3e",
            "729ae759d1d74415992bd142b3d10ef4",
            "76a18751e8264fc884650e571557aac7",
            "490812c0d31f476b93394850fdf6eed4",
            "c343294c4b6b442b8f4c9edddc01c64e",
            "30cc9b00eb7a466098bb96713e0b64cb",
            "c8ae1217e9d34e989a9cc24b076a70f7",
            "42c44b8ca1c84832a04364595845edac",
            "6d1714472aa64270861a18622e0ebad4",
            "9c2f7c116d2d4acd886a55b8a55ff50e",
            "e261059ed3814147837995d964394a34",
            "1d0a3a51d57648ac9e48106a6d53e870",
            "674b0806798d4c3bbb5f3db15a09e52e",
            "b05a6dea9aea4b95b5fe5f85e61a7fc4",
            "8822af0cea3641bbb479ca6fd66fd3fb",
            "6a95c10b053140efbd5976c2a52055b1",
            "104a6d45191a4279ad8426d561ae2edb",
            "5c97f0b15bab4e0f85531c85fc65f285",
            "b8472c013bd14f46aa1556ab7a5dad48",
            "a6e190e602104772b477deef83768626",
            "888bd7af225f41ab946cd72def7ae508",
            "d0d51a0221f14d65973a61ead2e77e83",
            "aad31a2b19294f26a6f33ad3bdbdd8a9",
            "1919ecf516274d44b6dd389e662a3bc3",
            "c1eaeec9bd5e4bdca568b37be92d790f"
          ]
        },
        "id": "8lcZg4veLSMh",
        "outputId": "b832c208-af1e-4dd9-bed9-7c93a1b346f2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    RobertaTokenizer, RobertaForMaskedLM, DataCollatorForLanguageModeling,\n",
        "    TrainingArguments, Trainer\n",
        ")\n",
        "\n",
        "# ========== 1. 載入 ELCo 資料 ==========\n",
        "elco_df = pd.read_csv(\"/content/ELCo/ELCo.csv\")\n",
        "\n",
        "# 建立多樣化 MLM 樣式句子\n",
        "templates = [\n",
        "    lambda em, en: f\"{em} usually means {en}.\",\n",
        "    lambda em, en: f\"When people use {em}, they are likely feeling {en}.\",\n",
        "    lambda em, en: f\"{em} is often used to show {en}.\",\n",
        "    lambda em, en: f\"Using {em} might suggest that someone is experiencing {en}.\",\n",
        "    lambda em, en: f\"The emoji {em} stands for {en} in many cases.\",\n",
        "]\n",
        "elco_df['text'] = [random.choice(templates)(em, en) for em, en in zip(elco_df[\"EM\"], elco_df[\"EN\"])]\n",
        "\n",
        "# ========== 2. 切分 train / val ==========\n",
        "train_texts, val_texts = train_test_split(elco_df['text'], test_size=0.1, random_state=42)\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': train_texts}))\n",
        "val_dataset = Dataset.from_pandas(pd.DataFrame({'text': val_texts}))\n",
        "\n",
        "# ========== 3. Tokenizer ==========\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "def tokenize_mlm(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_mlm, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_mlm, batched=True)\n",
        "\n",
        "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "# ========== 4. 訓練模型 ==========\n",
        "mlm_model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "mlm_args = TrainingArguments(\n",
        "    output_dir=\"./roberta_elco_mlm\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    learning_rate=1e-4,\n",
        "    eval_strategy=\"epoch\",  # ✅ 每個 epoch 都 evaluate\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "mlm_trainer = Trainer(\n",
        "    model=mlm_model,\n",
        "    args=mlm_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "mlm_trainer.train()\n",
        "\n",
        "# ========== 5. Evaluate：計算 Perplexity ==========\n",
        "eval_results = mlm_trainer.evaluate()\n",
        "perplexity = math.exp(eval_results[\"eval_loss\"])\n",
        "print(f\"🔍 Evaluation Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "# ========== 6. 儲存模型 ==========\n",
        "mlm_model.save_pretrained(\"./roberta_elco_mlm\")\n",
        "tokenizer.save_pretrained(\"./roberta_elco_mlm\")\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    RobertaTokenizer, RobertaForSequenceClassification,\n",
        "    TrainingArguments, Trainer, EvalPrediction\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# ========== 1. 讀入資料集 ==========\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/____YouTube_Emote____.csv\")\n",
        "df = df.dropna(subset=[\"premise\", \"hypothesis\", \"label\"])  # 確保無缺漏資料\n",
        "\n",
        "# 切分訓練 / 驗證集\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"label\"])\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# ========== 2. Tokenizer ==========\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"./roberta_elco_mlm\")\n",
        "\n",
        "def tokenize_emote(example):\n",
        "    return tokenizer(example[\"premise\"], example[\"hypothesis\"],\n",
        "                     truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_emote, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_emote, batched=True)\n",
        "\n",
        "for dataset in [tokenized_train, tokenized_val]:\n",
        "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
        "    dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# ========== 3. 載入 MLM 預訓練模型並 fine-tune ==========\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"./roberta_elco_mlm\", num_labels=2)\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds),\n",
        "        \"recall\": recall_score(labels, preds)\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./roberta_emote_finetune_youtube\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=5,\n",
        "    per_device_eval_batch_size=16,\n",
        "    eval_strategy=\"epoch\",  # ✅ 每個 epoch 都 evaluate\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=300,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ========== 4. 訓練與評估 ==========\n",
        "trainer.train()\n",
        "eval_result = trainer.evaluate()\n",
        "print(\"📊 Evaluation Results:\", eval_result)\n",
        "\n",
        "# ========== 5. 儲存模型 ==========\n",
        "model.save_pretrained(\"./roberta_emote_finetune\")\n",
        "tokenizer.save_pretrained(\"./roberta_emote_finetune\")\n",
        "\n",
        "\n",
        "# ✅ STEP 3: YouTube 留言情緒分析（emoji2vec + emoji位置）\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from gensim.models import KeyedVectors\n",
        "from transformers import RobertaTokenizer, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# 載入 emoji2vec 向量模型\n",
        "emoji_model = KeyedVectors.load_word2vec_format(\"/content/emoji2vec/pre-trained/emoji2vec.bin\", binary=True)\n",
        "\n",
        "# 擷取 emoji 向量平均\n",
        "def get_emoji_vec(text):\n",
        "    emojis = re.findall(r\"[\\U00010000-\\U0010ffff]\", text)\n",
        "    if not emojis: return np.zeros(emoji_model.vector_size)\n",
        "    vecs = [emoji_model[e] for e in emojis if e in emoji_model]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(emoji_model.vector_size)\n",
        "\n",
        "# 判斷 emoji 是否出現在留言後半段\n",
        "def get_emoji_position(text):\n",
        "    emojis = re.findall(r\"[\\U00010000-\\U0010ffff]\", text)\n",
        "    if not emojis: return 0  # 無 emoji 則預設為前段\n",
        "    emoji_str = ''.join(emojis)\n",
        "    idx = text.find(emoji_str)\n",
        "    return 1 if idx / max(1, len(text)) > 0.5 else 0\n",
        "\n",
        "# 載入資料\n",
        "yt_train = pd.read_csv('/content/drive/MyDrive/final_balanced_training_set(1).csv')\n",
        "yt_test = pd.read_csv('/content/drive/MyDrive/final_test_set_processed(1).csv')\n",
        "label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "yt_train[\"Sentiment\"] = yt_train[\"Sentiment\"].map(label_map)\n",
        "yt_test[\"Sentiment\"] = yt_test[\"Sentiment\"].map(label_map)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "class YouTubeEmojiDataset(TorchDataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = row['CommentText']\n",
        "        label = row['Sentiment']\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
        "        emoji_vec = get_emoji_vec(text)\n",
        "        emoji_pos = get_emoji_position(text)\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long),\n",
        "            'emoji_vec': torch.tensor(emoji_vec, dtype=torch.float32),\n",
        "            'emoji_pos': torch.tensor(emoji_pos, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class RobertaWithEmoji(nn.Module):\n",
        "    def __init__(self, model_path, emoji_dim=300, pos_dim=16, num_labels=3):\n",
        "        super().__init__()\n",
        "        from transformers import RobertaModel\n",
        "        self.roberta = RobertaModel.from_pretrained(model_path)\n",
        "        self.emoji_proj = nn.Linear(emoji_dim, 768)\n",
        "        self.pos_embedding = nn.Embedding(2, pos_dim)\n",
        "        self.classifier = nn.Linear(768 + 768 + pos_dim, num_labels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emoji_vec, emoji_pos, labels=None):\n",
        "        roberta_out = self.roberta(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "        emoji_embed = self.emoji_proj(emoji_vec)\n",
        "        pos_embed = self.pos_embedding(emoji_pos)\n",
        "        combined = torch.cat([roberta_out, emoji_embed, pos_embed], dim=1)\n",
        "        logits = self.classifier(self.dropout(combined))\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
        "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
        "        'emoji_vec': torch.stack([b['emoji_vec'] for b in batch]),\n",
        "        'emoji_pos': torch.stack([b['emoji_pos'] for b in batch]),\n",
        "        'labels': torch.stack([b['labels'] for b in batch])\n",
        "    }\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": f1_score(labels, preds, average='macro'),\n",
        "        \"precision\": precision_score(labels, preds, average='macro'),\n",
        "        \"recall\": recall_score(labels, preds, average='macro'),\n",
        "        \"auc\": roc_auc_score(label_binarize(labels, classes=[0,1,2]), logits, average='macro', multi_class='ovr')\n",
        "    }\n",
        "\n",
        "# 準備訓練\n",
        "train_set = YouTubeEmojiDataset(yt_train, tokenizer)\n",
        "test_set = YouTubeEmojiDataset(yt_test, tokenizer)\n",
        "model = RobertaWithEmoji(\"./roberta_emote_finetune\")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./roberta_youtube_sentiment\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=test_set,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "# ========== 預測並匯出結果 ==========\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# 切換到評估模式\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 準備 Dataloader\n",
        "test_loader = DataLoader(test_set, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "# 儲存欄位\n",
        "texts = yt_test['CommentText'].tolist()\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "emoji_lists = []\n",
        "\n",
        "# 預測 loop\n",
        "for i, batch in enumerate(tqdm(test_loader, desc=\"🔍 Predicting\")):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    emoji_vec = batch['emoji_vec'].to(device)\n",
        "    emoji_pos = batch['emoji_pos'].to(device) # Add this line to get emoji_pos from the batch\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Pass emoji_pos to the model's forward method\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, emoji_vec=emoji_vec, emoji_pos=emoji_pos)\n",
        "        logits = outputs[\"logits\"]\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "    true_labels.extend(labels.cpu().tolist())\n",
        "    pred_labels.extend(preds.cpu().tolist())\n",
        "\n",
        "# 抽取 emoji 函數\n",
        "def extract_emojis(text):\n",
        "    return ' '.join(re.findall(r\"[^\\w\\s,.!?\\'\\\"@#$%^&*()<>+=:;~`]+\", text))\n",
        "\n",
        "emoji_lists = [extract_emojis(t) for t in texts]\n",
        "\n",
        "# 建立 DataFrame 並匯出\n",
        "df_result = pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"pred_label\": pred_labels,\n",
        "    \"emoji_list\": emoji_lists\n",
        "})\n",
        "\n",
        "df_result.to_csv(\"youtube_emoji_sentiment_pretrain+position.csv\", index=False)\n",
        "print(\"✅ 匯出完成：youtube_emoji_sentiment_pretrain+position.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JonD5QGK2dh"
      },
      "source": [
        "# pretrain+position\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 1000,
          "referenced_widgets": [
            "74aa42b17acc40e18b3bb3d8216bf998",
            "4a3210290d5448d1af7543e0bbde3264",
            "abe08e28b62244bf9f05777ed9cbbbe6",
            "619a391c235f453fbb1756642a4f9c7f",
            "aef95cf7cd314024bace8ad51bf6b942",
            "582d28f51c484476969757044b94c873",
            "6821f7cca7fd4b8c802f20f3f5a9f0c8",
            "a5f52caa9d454cd5a5cc14210ffe659b",
            "016b37427b7847078bda5df9b12bd077",
            "0204487569d74ecda08fb7c21a12f542",
            "5fb346de888c4f6ea8bba5b5c2ec9d25",
            "3c45b00921e84d1e92d1fa10c37611f5",
            "62f806cc078043d5bd3b1aae4b99cb8f",
            "f5b4852548aa41b688e790166a06c839",
            "101ecfc44ea44e6e9dc3b7c31339e110",
            "59feba973b9a4b388fc7f1d54c23b954",
            "06c520965aaa46a195824ef010035a9a",
            "be006ceb8eff4dd2acc344d2d487fb19",
            "0508725f644e434bbf6fc84753308fb8",
            "4f41e65eea144e7a94ec62d6b9be9c3e",
            "729ae759d1d74415992bd142b3d10ef4",
            "76a18751e8264fc884650e571557aac7",
            "490812c0d31f476b93394850fdf6eed4",
            "c343294c4b6b442b8f4c9edddc01c64e",
            "30cc9b00eb7a466098bb96713e0b64cb",
            "c8ae1217e9d34e989a9cc24b076a70f7",
            "42c44b8ca1c84832a04364595845edac",
            "6d1714472aa64270861a18622e0ebad4",
            "9c2f7c116d2d4acd886a55b8a55ff50e",
            "e261059ed3814147837995d964394a34",
            "1d0a3a51d57648ac9e48106a6d53e870",
            "674b0806798d4c3bbb5f3db15a09e52e",
            "b05a6dea9aea4b95b5fe5f85e61a7fc4",
            "8822af0cea3641bbb479ca6fd66fd3fb",
            "6a95c10b053140efbd5976c2a52055b1",
            "104a6d45191a4279ad8426d561ae2edb",
            "5c97f0b15bab4e0f85531c85fc65f285",
            "b8472c013bd14f46aa1556ab7a5dad48",
            "a6e190e602104772b477deef83768626",
            "888bd7af225f41ab946cd72def7ae508",
            "d0d51a0221f14d65973a61ead2e77e83",
            "aad31a2b19294f26a6f33ad3bdbdd8a9",
            "1919ecf516274d44b6dd389e662a3bc3",
            "c1eaeec9bd5e4bdca568b37be92d790f"
          ]
        },
        "id": "Qk5CxNuCK2di",
        "outputId": "b832c208-af1e-4dd9-bed9-7c93a1b346f2"
      },
      "outputs": [],
      "source": [
        "# ✅ STEP 3: YouTube 留言情緒分析（emoji2vec + emoji位置）\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from gensim.models import KeyedVectors\n",
        "from transformers import RobertaTokenizer, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# 載入 emoji2vec 向量模型\n",
        "emoji_model = KeyedVectors.load_word2vec_format(\"/content/emoji2vec/pre-trained/emoji2vec.bin\", binary=True)\n",
        "\n",
        "# 擷取 emoji 向量平均\n",
        "def get_emoji_vec(text):\n",
        "    emojis = re.findall(r\"[\\U00010000-\\U0010ffff]\", text)\n",
        "    if not emojis: return np.zeros(emoji_model.vector_size)\n",
        "    vecs = [emoji_model[e] for e in emojis if e in emoji_model]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(emoji_model.vector_size)\n",
        "\n",
        "# 判斷 emoji 是否出現在留言後半段\n",
        "def get_emoji_position(text):\n",
        "    emojis = re.findall(r\"[\\U00010000-\\U0010ffff]\", text)\n",
        "    if not emojis: return 0  # 無 emoji 則預設為前段\n",
        "    emoji_str = ''.join(emojis)\n",
        "    idx = text.find(emoji_str)\n",
        "    return 1 if idx / max(1, len(text)) > 0.5 else 0\n",
        "\n",
        "# 載入資料\n",
        "yt_train = pd.read_csv('/content/drive/MyDrive/final_balanced_training_set(1).csv')\n",
        "yt_test = pd.read_csv('/content/drive/MyDrive/final_test_set_processed(1).csv')\n",
        "label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "yt_train[\"Sentiment\"] = yt_train[\"Sentiment\"].map(label_map)\n",
        "yt_test[\"Sentiment\"] = yt_test[\"Sentiment\"].map(label_map)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "class YouTubeEmojiDataset(TorchDataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = row['CommentText']\n",
        "        label = row['Sentiment']\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
        "        emoji_vec = get_emoji_vec(text)\n",
        "        emoji_pos = get_emoji_position(text)\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long),\n",
        "            'emoji_vec': torch.tensor(emoji_vec, dtype=torch.float32),\n",
        "            'emoji_pos': torch.tensor(emoji_pos, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class RobertaWithEmoji(nn.Module):\n",
        "    def __init__(self, model_path, emoji_dim=300, pos_dim=16, num_labels=3):\n",
        "        super().__init__()\n",
        "        from transformers import RobertaModel\n",
        "        self.roberta = RobertaModel.from_pretrained(model_path)\n",
        "        self.emoji_proj = nn.Linear(emoji_dim, 768)\n",
        "        self.pos_embedding = nn.Embedding(2, pos_dim)\n",
        "        self.classifier = nn.Linear(768 + 768 + pos_dim, num_labels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emoji_vec, emoji_pos, labels=None):\n",
        "        roberta_out = self.roberta(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "        emoji_embed = self.emoji_proj(emoji_vec)\n",
        "        pos_embed = self.pos_embedding(emoji_pos)\n",
        "        combined = torch.cat([roberta_out, emoji_embed, pos_embed], dim=1)\n",
        "        logits = self.classifier(self.dropout(combined))\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
        "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
        "        'emoji_vec': torch.stack([b['emoji_vec'] for b in batch]),\n",
        "        'emoji_pos': torch.stack([b['emoji_pos'] for b in batch]),\n",
        "        'labels': torch.stack([b['labels'] for b in batch])\n",
        "    }\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": f1_score(labels, preds, average='macro'),\n",
        "        \"precision\": precision_score(labels, preds, average='macro'),\n",
        "        \"recall\": recall_score(labels, preds, average='macro'),\n",
        "        \"auc\": roc_auc_score(label_binarize(labels, classes=[0,1,2]), logits, average='macro', multi_class='ovr')\n",
        "    }\n",
        "\n",
        "# 準備訓練\n",
        "train_set = YouTubeEmojiDataset(yt_train, tokenizer)\n",
        "test_set = YouTubeEmojiDataset(yt_test, tokenizer)\n",
        "model = RobertaWithEmoji(\"./roberta_emote_finetune\")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./roberta_youtube_sentiment\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=test_set,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "# ========== 預測並匯出結果 ==========\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# 切換到評估模式\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 準備 Dataloader\n",
        "test_loader = DataLoader(test_set, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "# 儲存欄位\n",
        "texts = yt_test['CommentText'].tolist()\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "emoji_lists = []\n",
        "\n",
        "# 預測 loop\n",
        "for i, batch in enumerate(tqdm(test_loader, desc=\"🔍 Predicting\")):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    emoji_vec = batch['emoji_vec'].to(device)\n",
        "    emoji_pos = batch['emoji_pos'].to(device) # Add this line to get emoji_pos from the batch\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Pass emoji_pos to the model's forward method\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, emoji_vec=emoji_vec, emoji_pos=emoji_pos)\n",
        "        logits = outputs[\"logits\"]\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "    true_labels.extend(labels.cpu().tolist())\n",
        "    pred_labels.extend(preds.cpu().tolist())\n",
        "\n",
        "# 抽取 emoji 函數\n",
        "def extract_emojis(text):\n",
        "    return ' '.join(re.findall(r\"[^\\w\\s,.!?\\'\\\"@#$%^&*()<>+=:;~`]+\", text))\n",
        "\n",
        "emoji_lists = [extract_emojis(t) for t in texts]\n",
        "\n",
        "# 建立 DataFrame 並匯出\n",
        "df_result = pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"pred_label\": pred_labels,\n",
        "    \"emoji_list\": emoji_lists\n",
        "})\n",
        "\n",
        "df_result.to_csv(\"youtube_emoji_sentiment_pretrain+position.csv\", index=False)\n",
        "print(\"✅ 匯出完成：youtube_emoji_sentiment_pretrain+position.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgVKvpLfsIwq"
      },
      "source": [
        "# pretrain+position (deberta)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 1000,
          "referenced_widgets": [
            "76f74827d038448baa618c72cb26eecb",
            "96a45a2342144e22a03351851773b178",
            "701aaf3ef6f9403c821ca78f94b1e9e8",
            "fc08486ab99346f8a3aecde78f10cc78",
            "204b82f2086a45fa91e098998a8273cf",
            "67cc0643c97b4dbbbfe0308218a5050f",
            "f549293c47b74dd0950d4133cea98638",
            "29bb7f5ab3904846bd29351eaef8b8bf",
            "8fdb78fe6250467989c156346f505e49",
            "133c93242793456290bdf11a37d75c22",
            "63dbfeeaf69544cc86d6dc5456f30f7c",
            "c053959e4f614d53a3350c3cf7d6b7bb",
            "492791fb035b4130a04720deb620ec2a",
            "f4898ea0c7174fc6849b1d15fbf3561e",
            "551c17a1c96e425cb33edb6fa6b1e738",
            "e2732eb632644a738278580df2bca59f",
            "ef99d64946514979890f2e6eb1a47323",
            "02919504579840488b849e4bb36f8d9c",
            "ac4da87c157a47bfaa08a94a12376cc1",
            "7d8de042d6534354bb3c6a6d164d66a8",
            "86f9e9fa27ec4c96b3a030fd845ec9cd",
            "bd6e4f05ff8c4cc7a74f7f586276c3ad",
            "10e3330d479442d4aa053be798f098cd",
            "371f8bf92c014c8d8b36158fc6f7e066",
            "d81f82571d74423295c82853f938b48f",
            "c9fcf240edc34a7783577f7d9529f00e",
            "4acb00aa53e1445faf7bfd02cc64abba",
            "0d104ffc02af4219a35b1ecb8d85cfd3",
            "307b3514f04040749a412ddee0ad7fd3",
            "1048b53ff6044b1a8761cc3a8fd2834f",
            "4b4758d08183463988e1072042eec63a",
            "c61d1dde06e94433a659ee7c5ca6e3e5",
            "894bb889aa07466ea5bdfb72c799a22d",
            "09a072250b844a8fad87a985cbe94fd9",
            "1b32ae8d4f4e4f01954c27bb39044a8d",
            "366adc48d6fb45f3852a31d2512b113e",
            "7da42792902740e1a231094da2c66e05",
            "ba2a74e9eb024ffbabd74c1d9c3d03ee",
            "cd34d2c15fc64407b89335f4e1d45817",
            "f1b9c64069c0407f8a8e9c00e46f578b",
            "6225d45e6243465585d0183ff9931615",
            "8513793f688d4fc7919ac4d40dfa8f5a",
            "72b298413a9e4206878af82c150c5513",
            "0064267566724ec5a1ccfba26b315493",
            "f5ca03542ec443acb2f48e4014e963ea",
            "96a3cab28f01467b982e293a3e6263e9",
            "198da03633a143c0b9ef0049a630493a",
            "08643bb981bf4f9d8a6c5aaeae29d7cb",
            "0e1216a765654f60a3196f660d0a8469",
            "62ab35e2eebc424c81c4077a58443881",
            "b19f10e874d94f8683cc99a89533c1a8",
            "b879921cb401481c987068c5f3042f14",
            "f2072c358084481091447d7b4a3ddb93",
            "c4d6c16ee6794250a437a87adb2b8e93",
            "f7954d9de21d48a687fa780fcbfaace7",
            "f25a8b21d1954e23b9b4228b598ea404",
            "0dc36e83a8c048789a9d03657d568551",
            "6e3161d52adf46c6a194cba38f6b74eb",
            "d881dda723ef4d8f8455f9a8e271a6bc",
            "61ab9349fd2d429e9bbb3d47c746cb38",
            "de0a9ab443864b6bbf353e373b335c9d",
            "b9f9194355b541b2b0f7a053197f5871",
            "99bbaaf30d844977a72098df392ed65a",
            "6b89c772286f41248c303b719a95534b",
            "7f47e36f82e74aa3b75f939b412ea82a",
            "081fd3c9e1a34bf3b043bbed02440727",
            "3dbd1bdb4c4e49f593861da0f50f0b75",
            "2c6a95076e0240859d5e8281fbd4481b",
            "f443d95aa1d041d290778841d5381b9f",
            "eab28f8b7ca74f38ae1c93ad814ab1d9",
            "7387b7ce29164e51a35e0d4ed912861d",
            "d6803cf5dcd447ed98a87067f2f78ebe",
            "7103dd4bce684036b87fb7e3b8c13ecc",
            "28bb660d17284ecca6fff4f602ce617b",
            "73ee3ddd7e9d45e790eba76781d8d78d",
            "ba264f7ac3b24d428426bb34c6e3f6b5",
            "08e77a49a3af4e818f9e3368c6a86e23",
            "b03b1496455f4b529759b6f70e9e8163",
            "96d9ca6ca2bb4a458408a5d24f5be571",
            "4f1cad7b80d7400ba5e57a2aba9537b6",
            "ed88441373764e9082110ee2e0b7eb2f",
            "179881375b3542a898ad36da5778ad45",
            "d1f6d03a52814e979edb10a4b1ced1fe",
            "2cce48c4180240edbb9a0fafe6f95264",
            "1516c1e7c54c4e6db37ed68b175e55a4",
            "9acd259858c643b59e9f68608f43021d",
            "7287baf7399e4a41b38317bca77fb27e",
            "56c8a0f21b144a53bb0c1c21107803d2",
            "37c9da9a80aa406499baf373b98ca673",
            "5832a1a06d2848f79ddb957607b744f1",
            "c7f1d443dab041bca32246deecc2fc9f",
            "e18a7e5c1bff4594b4aaf5ae25a60e2e",
            "514a55a3620e4ee3a961246d0cdee3ae",
            "eafa9f07b2c9456a98c76eafc35a4cea",
            "68669501b4ed4a23ba06f136fd173661",
            "183812c0c8d6464fa3be20922a7ee5cd",
            "6cdccc623aa1432794fe8014f71853ac",
            "7909628a5d924a84ae062e4b49f0ee86",
            "cc2c89d7394c431ea1e97286782cdb64",
            "b6df0a34bf914e72b0206fee2538a365",
            "add67e0fc3ee4b1ba8df76adbd3d022f",
            "702a51a7a9564711b79894941ba64960",
            "7861f3c7257d49bc89ce9aebe12b1a29",
            "b9013bccf89a406bbb6d7aade88d7345",
            "59dc5eee91b84d94a2e05ee207188119",
            "e81ba1f6581440f9abb02699165dc155",
            "fce63b24f2824cca9ef4923b4e2f7662",
            "5187907b04f44a37a3bedc3d5f6b7721",
            "a00526d51d0f467c89504b0638f1759b",
            "feb7fc1c311c4b8bb3df6f90c0f75855"
          ]
        },
        "id": "VZZhBX7UsIwq",
        "outputId": "0872cb10-0f72-44e9-c15b-cac455bb4331"
      },
      "outputs": [],
      "source": [
        "# ✅ 整合後：使用 microsoft/deberta-base 實作完整流程\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from transformers import (\n",
        "    DebertaTokenizer, DebertaForMaskedLM, DebertaModel,\n",
        "    DebertaForSequenceClassification,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments, Trainer, EvalPrediction\n",
        ")\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from gensim.models import KeyedVectors\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====== STEP 1: MLM 預訓練 (ELCo) ======\n",
        "elco_df = pd.read_csv(\"/content/ELCo/ELCo.csv\")\n",
        "templates = [\n",
        "    lambda em, en: f\"{em} usually means {en}.\",\n",
        "    lambda em, en: f\"When people use {em}, they are likely feeling {en}.\",\n",
        "    lambda em, en: f\"{em} is often used to show {en}.\",\n",
        "    lambda em, en: f\"Using {em} might suggest that someone is experiencing {en}.\",\n",
        "    lambda em, en: f\"The emoji {em} stands for {en} in many cases.\"\n",
        "]\n",
        "elco_df['text'] = [random.choice(templates)(em, en) for em, en in zip(elco_df['EM'], elco_df['EN'])]\n",
        "train_texts, val_texts = train_test_split(elco_df['text'], test_size=0.1, random_state=42)\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': train_texts}))\n",
        "val_dataset = Dataset.from_pandas(pd.DataFrame({'text': val_texts}))\n",
        "\n",
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "def tokenize_mlm(example):\n",
        "    return tokenizer(example['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_mlm, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_mlm, batched=True)\n",
        "\n",
        "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "mlm_model = DebertaForMaskedLM.from_pretrained(\"microsoft/deberta-base\")\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "mlm_args = TrainingArguments(\n",
        "    output_dir=\"./deberta_elco_mlm\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    learning_rate=1e-4,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "mlm_trainer = Trainer(\n",
        "    model=mlm_model,\n",
        "    args=mlm_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "mlm_trainer.train()\n",
        "mlm_model.save_pretrained(\"./deberta_elco_mlm\")\n",
        "tokenizer.save_pretrained(\"./deberta_elco_mlm\")\n",
        "\n",
        "# ====== STEP 2: Entailment fine-tuning ======\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/____YouTube_Emote____.csv\").dropna()\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"label\"])\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "tokenizer = DebertaTokenizer.from_pretrained(\"./deberta_elco_mlm\")\n",
        "def tokenize_emote(example):\n",
        "    return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_emote, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_emote, batched=True)\n",
        "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
        "tokenized_val = tokenized_val.rename_column(\"label\", \"labels\")\n",
        "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "model = DebertaForSequenceClassification.from_pretrained(\"./deberta_elco_mlm\", num_labels=2)\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds),\n",
        "        \"recall\": recall_score(labels, preds)\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./deberta_emote_finetune\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=5,\n",
        "    per_device_eval_batch_size=16,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=300,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./deberta_emote_finetune\")\n",
        "\n",
        "# ====== STEP 3: YouTube 情緒分析 with emoji features ======\n",
        "def get_emoji_vec(text):\n",
        "    emojis = re.findall(r\"[\\U00010000-\\U0010ffff]\", text)\n",
        "    if not emojis: return np.zeros(300)\n",
        "    vecs = [emoji_model[e] for e in emojis if e in emoji_model]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(300)\n",
        "\n",
        "def get_emoji_position(text):\n",
        "    emojis = re.findall(r\"[\\U00010000-\\U0010ffff]\", text)\n",
        "    if not emojis: return 0\n",
        "    emoji_str = ''.join(emojis)\n",
        "    idx = text.find(emoji_str)\n",
        "    return 1 if idx / max(1, len(text)) > 0.5 else 0\n",
        "\n",
        "emoji_model = KeyedVectors.load_word2vec_format(\"/content/emoji2vec/pre-trained/emoji2vec.bin\", binary=True)\n",
        "yt_train = pd.read_csv('/content/drive/MyDrive/final_balanced_training_set(1).csv')\n",
        "yt_test = pd.read_csv('/content/drive/MyDrive/final_test_set_processed(1).csv')\n",
        "label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "yt_train[\"Sentiment\"] = yt_train[\"Sentiment\"].map(label_map)\n",
        "yt_test[\"Sentiment\"] = yt_test[\"Sentiment\"].map(label_map)\n",
        "\n",
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "class YouTubeEmojiDataset(TorchDataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = row['CommentText']\n",
        "        label = row['Sentiment']\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
        "        emoji_vec = get_emoji_vec(text)\n",
        "        emoji_pos = get_emoji_position(text)\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long),\n",
        "            'emoji_vec': torch.tensor(emoji_vec, dtype=torch.float32),\n",
        "            'emoji_pos': torch.tensor(emoji_pos, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class DebertaWithEmoji(nn.Module):\n",
        "    def __init__(self, model_path, emoji_dim=300, pos_dim=16, num_labels=3):\n",
        "        super().__init__()\n",
        "        self.deberta = DebertaModel.from_pretrained(model_path)\n",
        "        self.emoji_proj = nn.Linear(emoji_dim, 768)\n",
        "        self.pos_embedding = nn.Embedding(2, pos_dim)\n",
        "        self.classifier = nn.Linear(768 + 768 + pos_dim, num_labels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, emoji_vec, emoji_pos, labels=None):\n",
        "        deberta_out = self.deberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
        "        emoji_embed = self.emoji_proj(emoji_vec)\n",
        "        pos_embed = self.pos_embedding(emoji_pos)\n",
        "        combined = torch.cat([deberta_out, emoji_embed, pos_embed], dim=1)\n",
        "        logits = self.classifier(self.dropout(combined))\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
        "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
        "        'emoji_vec': torch.stack([b['emoji_vec'] for b in batch]),\n",
        "        'emoji_pos': torch.stack([b['emoji_pos'] for b in batch]),\n",
        "        'labels': torch.stack([b['labels'] for b in batch])\n",
        "    }\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": f1_score(labels, preds, average='macro'),\n",
        "        \"precision\": precision_score(labels, preds, average='macro'),\n",
        "        \"recall\": recall_score(labels, preds, average='macro'),\n",
        "        \"auc\": roc_auc_score(label_binarize(labels, classes=[0,1,2]), logits, average='macro', multi_class='ovr')\n",
        "    }\n",
        "\n",
        "train_set = YouTubeEmojiDataset(yt_train, tokenizer)\n",
        "test_set = YouTubeEmojiDataset(yt_test, tokenizer)\n",
        "model = DebertaWithEmoji(\"./deberta_emote_finetune\")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./deberta_youtube_sentiment\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=test_set,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "eval_result = trainer.evaluate()\n",
        "print(\"Evaluation:\", eval_result)\n",
        "\n",
        "# ========== 預測並匯出結果 ==========\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# 切換到評估模式\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 準備 Dataloader\n",
        "test_loader = DataLoader(test_set, batch_size=16, collate_fn=collate_fn)\n",
        "\n",
        "# 儲存欄位\n",
        "texts = yt_test['CommentText'].tolist()\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "emoji_lists = []\n",
        "\n",
        "# 預測 loop\n",
        "for i, batch in enumerate(tqdm(test_loader, desc=\"🔍 Predicting\")):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    emoji_vec = batch['emoji_vec'].to(device)\n",
        "    emoji_pos = batch['emoji_pos'].to(device) # Add this line to get emoji_pos from the batch\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Pass emoji_pos to the model's forward method\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, emoji_vec=emoji_vec, emoji_pos=emoji_pos)\n",
        "        logits = outputs[\"logits\"]\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "    true_labels.extend(labels.cpu().tolist())\n",
        "    pred_labels.extend(preds.cpu().tolist())\n",
        "\n",
        "# 抽取 emoji 函數\n",
        "def extract_emojis(text):\n",
        "    return ' '.join(re.findall(r\"[^\\w\\s,.!?\\'\\\"@#$%^&*()<>+=:;~`]+\", text))\n",
        "\n",
        "emoji_lists = [extract_emojis(t) for t in texts]\n",
        "\n",
        "# 建立 DataFrame 並匯出\n",
        "df_result = pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"pred_label\": pred_labels,\n",
        "    \"emoji_list\": emoji_lists\n",
        "})\n",
        "\n",
        "df_result.to_csv(\"youtube_emoji_sentiment_pretrain+position.csv\", index=False)\n",
        "print(\"✅ 匯出完成：youtube_emoji_sentiment_pretrain+position.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}